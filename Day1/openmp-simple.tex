	\subsection{Introduction}


\begin{frame}
  \begin{center}
    {\includegraphics[height=2cm]{Day1/images/logo_OpenMP.png}}

    \textbf{Lecture based on specifications ver 3.1}

  \end{center}
\end{frame}

\begin{frame}
  \frametitle{Releases history, present and future}
  \begin{itemize}
  \item{October 1997: Fortran version 1.0 }
  \item{Late 1998: C/C++ version 1.0 }
  \item{June 2000: Fortran version 2.0 }
  \item{April 2002: C/C++ version 2.0 }
  \item{June 2005: Combined C/C++ and Fortran version 2.5 }
  \item{May 2008: Combined C/C++ and Fortran version  3.0}
  \item{\textbf{July 2011: Combined C/C++ and Fortran version  3.1}}
  \item{July 2013: Combined C/C++ and Fortran version 4.0 }
  \item{November 2015: Combined C/C++ and Fortran version 4.5 }
  \item{November 2018: Combined C/C++ and Fortran version 5.0 }
  \end{itemize}
\end{frame}

% \subsubsection{threading concepts, terminology (OpenMP, tasking, data, implementation)}

\begin{frame}
  \frametitle{Terminology}
  \begin{itemize}
  \item{\textbf{thread :} an execution entity with a stack and a static memory (\textit{threadprivate memory})}
  \item{\textbf{OpenMP thread :} a \textit{thread} managed by the OpenMP runtime}
  \item{\textbf{thread-safe routine :} a routine that can be executed concurrently}
  \item{\textbf{processor :} an HW unit on which one or more \textit{OpenMP thread} can execute}
%  \item{\textbf{device :} an implementation defined logical execution engine}
  \end{itemize}
\end{frame}

\subsubsection{Models}

\begin{frame}
  \frametitle{Execution and memory models}
  \begin{itemize}
  \item{Execution model : fork-join}
  \item{One heavy thread (process) per program (initial thread)}
  \item{leightweigt threads for parallel regions. threads are assigned to cores by the OS}
  \item{No implicit synchronization (except at the beginning and at the end of a parallel region)}
  \item{Shared Memory with shared variables}
  \item{Private Memory per thread with threadprivate variables}
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Memory model (simplified)}
  \begin{center}
    {\includegraphics[width=\textwidth]{Day1/images/memory-model-simplified.pdf}}
  \end{center}
\end{frame}

\begin{frame}
  \frametitle{Execution model (simplified)}
  \begin{center}
    {\input{Day1/images/execution-model-simplified.tex}}
  \end{center}
\end{frame}

\subsubsection{Remarks}

\begin{frame}
  \frametitle{OpenMP and MPI/pthreads	}
  \begin{itemize}
  \item{\textbf{OpenMP} $\neq$ OpenMPI}
  \item{All what you can do with OpenMP can be done with MPI and/or pthreads}
    % \item{Memory issue}
  \item{easier \textbf{BUT} data coherence/consistency}
    % \item{In fact: \textbf{no} easy parallel paradigm exists}
  \end{itemize}
\end{frame}

\subsection{Directives}

\subsubsection{format \& conditional compilation}

\begin{frame}[containsverbatim]
  \frametitle{Syntax in C}
  % \begin{itemize}
  % \item{OpenMP directives are written as pragmas: \texttt{\#pragma omp}}
  % \item{Use the conditional compilation flag \texttt{\#if defined \_OPENMP} for the preprocessor}
  % \end{itemize}

  \begin{block}{}
    OpenMP directives are written as pragmas: \texttt{\#pragma omp}
  \end{block}

  \begin{block}{}
    Use the conditional compilation flag \texttt{\#if defined \_OPENMP} for the preprocessor
  \end{block}

  Compilation using the GNU gcc or Intel compiler:
\begin{verbatim}
gcc -fopenmp ex1.c -o ex1
\end{verbatim}
\end{frame}


\begin{frame}[containsverbatim]
  \frametitle{Hello World in C}

  \begin{lstlisting}[language=C++,frame=lines]
#include <stdio.h>
#include <omp.h>
int main(int argc, char *argv[]) {
   int myrank=0;
   int mysize=1;
#if defined (_OPENMP)
#pragma omp parallel default(shared) private(myrank, mysize)
{
   mysize = omp_get_num_threads();
   myrank = omp_get_thread_num();
#endif
   printf("Hello from thread %d out of %d\n", myrank, mysize);
#if defined (_OPENMP)
}
#endif
   return 0;
}
\end{lstlisting}
% (Source file: \texttt{ex1.c})
\end{frame}

\begin{frame}[containsverbatim]
  \frametitle{Syntax in Fortran 90}
  % \begin{itemize}
  % \item{OpenMP directives are written as comments: \texttt{!\$omp omp}}
  % \item{Sentinels \texttt{!\$} are authorized for conditional compilation  (preprocessor) }
  % \end{itemize}

  \begin{block}{}
    OpenMP directives are written as comments: \texttt{!\$omp omp}
  \end{block}
  \begin{block}{}
    Sentinels \texttt{!\$} are authorized for conditional compilation  (preprocessor)
  \end{block}


  Compilation using the GNU gfortran or Intel ifort compiler:
\begin{verbatim}
gfortran -fopenmp ex1.f90 -o ex1
\end{verbatim}
\end{frame}



\begin{frame}[containsverbatim]
  \frametitle{Hello World in Fortran 90}

  \begin{lstlisting}[language=Fortran,frame=lines]
program ex1
   implicit none
   integer :: myrank, mysize
!$ integer, external :: omp_get_num_threads, omp_get_thread_num
   myrank=0
   mysize=1
!$omp   parallel default(shared) private(myrank, mysize)
!$      mysize = omp_get_num_threads()
!$      myrank = omp_get_thread_num()
   print *, "Hello from thread",myrank,"out of",mysize
!$omp   end parallel
end program ex1
\end{lstlisting}
% $ This comment is just for the colors in emacs...
\end{frame}

\begin{frame}[containsverbatim]
  \frametitle{Number of concurrent threads}

  The number of threads is specified in a hardcoded way ($omp\_set\_num\_threads()$) or via an environment variable.
  \\~\\

  BASH-like shells :

\begin{verbatim}
export OMP_NUM_THREADS=4
\end{verbatim}

  CSH-like shells :

\begin{verbatim}
setenv OMP_NUM_THREADS 4
\end{verbatim}
\end{frame}


\begin{frame}[containsverbatim]
  \frametitle{Components of OpenMP}
  \begin{itemize}
  \item{Compiler directives (written as comments) that allow work sharing, synchronization and data scoping}
  \item{A runtime library (libomp.so) that contains informal, data access and synchronization directives}
  \item{Environment variables}
  \end{itemize}
\end{frame}


\subsubsection{The \texttt{parallel} construct}


\begin{frame}[containsverbatim]
  \frametitle{The \texttt{parallel} construct}

  \begin{exampleblock}{Syntax}
    This is the mother of all constructs in OpenMP. It starts a parallel execution.
    \begin{lstlisting}[language=C,frame=lines]
#pragma omp parallel [clause[[,] clause]...]
{
   structured-block
}
\end{lstlisting}
    where \textit{clause} is one of the following:
    \begin{itemize}
    \item \textbf{\texttt{if}} or \textbf{\texttt{num\_threads}} : conditional clause
    \item \textbf{\texttt{default(private (only Fortran) | firstprivate (only Fortran) | shared | none)}} : default data scoping
    \item \textbf{\texttt{private(\textit{list})}}, \textbf{\texttt{firstprivate(\textit{list})}}, \textbf{\texttt{shared(\textit{list})}} or \textbf{\texttt{copyin(\textit{list})}} : data scoping
    \item \textbf{\texttt{reduction(\textit{\{ operator | intrinsic\_procedure\_name \} : list})}}
    \end{itemize}
  \end{exampleblock}
\end{frame}


\begin{frame}[containsverbatim]
  \frametitle{Data scoping}
  What is data scoping ?
  \begin{itemize}
  \item{most common source of errors}
  \item{determine which variables are {\bf private} to a thread, which are {\bf shared} among all the threads}
  \item{In case of a private variable, what is its value when entering the
      parallel region {\bf firstprivate}, what is its value when leaving the
      parallel region {\bf lastprivate}}
  \item The default scope (if none are specified) is \textbf{shared}
  \item{most difficult part of OpenMP}
  \end{itemize}
\end{frame}


\begin{frame}[fragile]
  \frametitle{The data sharing-attributes \texttt{shared} and \texttt{private}}
  \begin{exampleblock}{Syntax}
These attributes determines the scope (visibility) of a single or list of variables
\begin{lstlisting}[language=C,frame=lines]
shared(list1) private(list2)
\end{lstlisting}

\begin{itemize}
\item{The \verb+private+ attribute : the data is private to each thread and non-initiatilized. Each thread has its own copy. Example : \verb+#pragma omp parallel private(i)+}
\item{The \verb+shared+ attribute : the data is shared among all the threads. It is accessible (and non-protected) by all the threads simultaneously. Example : \verb+#pragma omp parallel shared(array)+}
\end{itemize}

\end{exampleblock}

\end{frame}



\begin{frame}[containsverbatim]
\frametitle{The data sharing-attributes \texttt{firstprivate} and \texttt{lastprivate}}
\begin{exampleblock}{Syntax}
These clauses determines the attributes of the variables within a parallel/parallel for region:
\begin{lstlisting}[language=C,frame=lines]
firstprivate(list1) lastprivate(list2)
\end{lstlisting}
\begin{itemize}
\item{The \texttt{firstprivate} like {\tt private} but initialized to the value before the parallel region}
\item{The \texttt{lastprivate}  like {\tt private} but the value is updated after the parallel for}
\end{itemize}
\end{exampleblock}
%\begin{alertblock}{}
%\textbf{Fortran only !}
%\end{alertblock}
\end{frame}




\subsubsection[Worksharing constructs]{worksharing constructs ("subsubsections", "single", "workshare")}



\begin{frame}[containsverbatim]
\frametitle{Worksharing constructs}

\begin{block}{}
Worksharing constructs are possible in three ``flavours'' :
\begin{itemize}
\item{\textbf{\texttt{sections}} construct}
\item{\textbf{\texttt{single}} construct}
%\item{\textbf{\texttt{master}} construct}
\item{\textbf{\texttt{workshare}} construct (only in Fortran)}
\end{itemize}
\end{block}

\end{frame}


\begin{frame}[containsverbatim]
\frametitle{The \texttt{single} construct}

\begin{exampleblock}{Syntax}
\begin{lstlisting}[language=C,frame=lines]
#pragma omp single [clause[[,] clause] ...]
{
   structured-block
}
\end{lstlisting}
where \textit{clause} is one of the following:
\begin{itemize}
\item{\textbf{\texttt{private(\textit{list})}}, \textbf{\texttt{firstprivate(\textit{list})}}}
\end{itemize}
\end{exampleblock}

\begin{block}{}
Only one thread (usualy the first entering thread) executes the \textbf{\texttt{single}} region. The others wait for completion, except if the \textbf{\texttt{nowait}} clause has been activated
\end{block}

\end{frame}





\subsubsection[Loops]{loops}

\begin{frame}[containsverbatim]
\frametitle{The \texttt{for} directive}

\begin{block}{}
Parallelization of the following loop
\end{block}

\begin{exampleblock}{Syntax}
\begin{lstlisting}[language=C,frame=lines]
#pragma omp for [clause[[,] clause] ... ]
   for-loop
\end{lstlisting}
where \textit{clause} is one of the following:
\begin{itemize}
\item{\textbf{\texttt{schedule(\textit{kind[, chunk\_size]})}}}
\item{\textbf{\texttt{collapse(\textit{n})}}}
\item{\textbf{\texttt{ordered}}}
\item{\textbf{\texttt{private(\textit{list})}}, \textbf{\texttt{firstprivate(\textit{list})}}, \textbf{\texttt{lastprivate(\textit{list})}},\texttt{reduction()} }
\end{itemize}
\end{exampleblock}
\end{frame}


\begin{frame}[containsverbatim]
\frametitle{The \texttt{reduction(...)} clause (Exercise)}
\begin{block}{How to deal with}
\vspace{-.5cm}
\begin{verbatim}
vec = (int*) malloc (size_vec*sizeof(int));
global_sum = 0;
for (i=0;i<size_vec;i++){
   global_sum += vec[i];
}
\end{verbatim}
\end{block}

\vspace{-.5cm}
\begin{block}{A solution with the \texttt{reduction(...)} clause}
\vspace{-.5cm}

\begin{verbatim}
vec = (int*) malloc (size_vec*sizeof(int));
global_sum = 0;
#pragma omp parallel for reduction(+:global_sum)
   for (i=0;i<size_vec;i++){
      global_sum += vec[i];
   }
\end{verbatim}
But other solutions exist !
\end{block}
\end{frame}


\begin{frame}[containsverbatim]
\frametitle{The \texttt{schedule} clause}

\begin{block}{}
Load-balancing
\end{block}

\begin{center}
\begin{tabular}{|l|l|}
\hline
  \textbf{clause} & \textbf{behavior}  \\
\hline
\hline
\textit{schedule(static [, chunk\_size])} &
iterations divided in chunks sized \\
& \textit{chunk\_size} assigned to threads in \\
& a round-robin fashion.  \\
& If \textit{chunk\_size} not specified \\
& system decides. \\
\hline

\textit{schedule(dynamic [, chunk\_size])} &
iterations divided in chunks sized \\
& \textit{chunk\_size} assigned to threads \\
& when they request them until no \\
& chunk remains to be distributed. \\
& If \textit{chunk\_size} not specified \\
& default is 1. \\

\hline
\end{tabular}
\end{center}

\end{frame}


\begin{frame}[containsverbatim]
\frametitle{The \texttt{schedule} clause}

\begin{center}
\begin{tabular}{|l|l|}
\hline
  \textbf{clause} & \textbf{behavior}  \\
\hline
\hline
\textit{schedule(guided [, chunk\_size])}  &
iterations divided in chunks sized \\
& \textit{chunk\_size} assigned to threads \\
& when they request them. Size of  \\
& chunks is proportional to the  \\
& remaining unassigned chunks. \\
%& If \textit{chunk\_size} not specified \\
%& default is 1. \\
& By default the chunk size is approx \\
& loop$\_$count/number$\_$of$\_$threads. \\

%By default the chunk size is approximately 


\hline
\textit{schedule(auto)}  &
The decisions is delegated to the \\
 & compiler and/or the runtime system \\

\hline
\textit{schedule(runtime)}  &
The decisions is delegated to the \\
 & runtime system \\


\hline
\end{tabular}
\end{center}

\end{frame}


\begin{frame}[containsverbatim]
\frametitle{A parallel \texttt{for} example}


\begin{block}{How to...}
... parallelize the dense matrix multiplication $C = A B$ (triple for loop $C_{ij} = C_{ij} + A_{ik} B_{kj}$))
\end{block}

\end{frame}


\begin{frame}[containsverbatim]
\frametitle{A parallel \texttt{for} example}
\begin{lstlisting}[language=C,frame=lines]
   #pragma omp parallel shared(A,B,C) private(i,j,k,myrank)
   {
      myrank=omp_get_thread_num();
      mysize=omp_get_num_threads();
      chunk=(N/mysize);
      #pragma omp for schedule(static, chunk)
      for (i=0;i<N;i++){
         for (j=0;j<N;j++){
            for (k=0;k<N;k++){
               C[i][j]=C[i][j] + A[i][k]*B[k][j];
            }
         }
      }
   }
\end{lstlisting}
%Loop order is important !
\end{frame}

\begin{frame}[containsverbatim]
\frametitle{A parallel \texttt{for} example}

\begin{verbatim}
vkeller@mathicsepc13:~$ export OMP_NUM_THREADS=1
vkeller@mathicsepc13:~$ ./a.out
 [DGEMM] Compute time [s]   : 0.33388209342956
 [DGEMM] Performance  [GF/s]: 0.59901385529736
 [DGEMM] Verification       : 2000000000.00000
vkeller@mathicsepc13:~$ export OMP_NUM_THREADS=2
vkeller@mathicsepc13:~$ ./a.out
 [DGEMM] Compute time [s]   : 0.18277192115783
 [DGEMM] Performance  [GF/s]: 1.09425998661625
 [DGEMM] Verification       : 2000000000.00000
vkeller@mathicsepc13:~$ export OMP_NUM_THREADS=4
vkeller@mathicsepc13:~$ ./a.out
 [DGEMM] Compute time [s]   : 9.17780399322509E-002
 [DGEMM] Performance  [GF/s]: 2.17917053085506
 [DGEMM] Verification       : 2000000000.00000
\end{verbatim}

\end{frame}




\begin{frame}
\frametitle{The \texttt{collapse} clause}

\begin{block}{Intel view}
Use the collapse clause to increase the total number of iterations that will be partitioned across the available number of OMP threads by reducing the granularity of work to be done by each thread.

You can improve performance by avoiding use of the collapsed-loop indices (if possible) inside the collapse loop-nest (since the compiler has to recreate them from the collapsed loop-indices using divide/mod operations AND the uses are complicated enough that they don't get dead-code-eliminated as part of compiler optimizations)
\end{block}

\end{frame}

\begin{frame}[containsverbatim]
\frametitle{A \texttt{collapse} directive example}

\begin{lstlisting}[language=C,frame=lines]
#pragma omp parallel for collapse(2) shared(A) private(k,l)
   for (k=0;k<kmax;k++) {
      for (l=0;l<lmax;l++){
         do_some_work(&A,k,l,N);
      }
   }
\end{lstlisting}
where \texttt{do\_some\_work(A,k,l,N)} looks like:
\begin{lstlisting}[language=C,frame=lines]
   for(i=0;i<N;i++) {
      for (j=0;j<N;j++) {
         A[i][j] = A[i][j]*s+A[i][j]*t
      }
   }
\end{lstlisting}
\end{frame}

\begin{frame}[containsverbatim]
\frametitle{A \texttt{collapse} directive example [output]}
\begin{block}{}
Here we compare the collapsed result with the standard parallel loop (on \texttt{k})
\end{block}

%\begin{table}
%\begin{center}
\begin{tabular}{|l|l|l|l|l|l|}
\hline
 \textbf{OMP\_NUM\_THREADS} & \textbf{1} & \textbf{2} & \textbf{4} & \textbf{8} & \textbf{16}\\
\hline
\hline
standard // loop &2.17 &1.69 &1.69 &1.44 &1.22 \\
\hline
collapsed(2) // loop &2.13 &1.60 &1.02 &0.83 &0.70 \\
\hline
\end{tabular}
%\end{center}
%\caption{\texttt{N=20000}, time is in seconds}
%\end{table}

\texttt{N=20000}, time is in seconds

\begin{block}{Remark}
It is mandatory that the \textit{\texttt{n-}}collapsed loops are perfectly nested and with a rectangular shape (nothing like \texttt{do i=1,N ... do j=1,f(i)}) and that their upper limits are ``small''.
\end{block}



\end{frame}




\subsubsection{tasking constucts ("task", "taskyield")}

\begin{frame}[containsverbatim]
\frametitle{The \texttt{task} directive}
\begin{center}
    {\input{Day1/images/Fork_join.tex}}
\end{center}
Source : wikipedia.org
\end{frame}


\begin{frame}[containsverbatim]
\frametitle{The \texttt{task} directive}

\begin{exampleblock}{What is an OpenMP task ?}
\begin{itemize}
\item{Offers a solution to parallelize irregular problems (unbounded loops, recursives, master/slave schemes, etc..)}
\item{OpenMP tasks are composed of
        \begin{itemize}
        \item{\textbf{code} : what will be executed}
        \item{\textbf{data} : initialized at task creation time }
        \item{\textbf{ICV's} : Internal Control Variables }
        \end{itemize}
}
\end{itemize}
\end{exampleblock}

\begin{exampleblock}{Synchronization}
\begin{itemize}
\item{All tasks created by a thread of a team are garanteed to be completed at
    thread exit (end of block)}
\item{Within a task group, it is possible to synchronize through \texttt{\#pragma
      omp taskwait}}
\end{itemize}
\end{exampleblock}
\end{frame}



\begin{frame}[containsverbatim]
\frametitle{The \texttt{task} directive}

\begin{exampleblock}{Execution model}
\begin{itemize}
\item{A task \texttt{t} is executed by the thread \texttt{T} of the team that generated it. Immediately or not (depends on the implementation)}
\item{A thread \texttt{T} can suspend/resume/restart a task \texttt{t}}
\item{Tasks are \textbf{tied} by default:
        \begin{itemize}
        \item{tied tasks are executed by the same thread}
        \item{tied tasks have scheduling restrictions (deterministic creation, synchronization, destruction)}
        \end{itemize}
}
\item{It is possible to untie tasks with the directive \texttt{untied}}
\end{itemize}
\end{exampleblock}


\end{frame}



\begin{frame}[containsverbatim]
\frametitle{Tasks remarks}

\begin{itemize}
        \item{targeted for many-cores co-processors (like Intel Phi)}
        \item{Can be used to solve non symetrical problems and algorithms}
        \item{Avoid them on a multi-core CPU}
\end{itemize}

\end{frame}



\subsubsection[Master and Synchronization constructs]{Master and Synchronization constructs ("master", "critical", "barrier", "taskwait", "atomic", "flush", "ordered")}


\begin{frame}
\frametitle{Synchronization}
\begin{block}{Synchronization constructs}
Those directives are sometimes mandatory:
\begin{itemize}
        \item{\texttt{master} : region is executed by the master thread only }
        \item{\texttt{critical} : region is executed by only one thread at a time }
        \item{\texttt{barrier} : all threads must reach this directive to continue}
        \item{\texttt{taskwait} : all tasks and childs must reach this directive to continue}
        \item{\texttt{atomic (read | write | update | capture)} : the associated storage location is accessed by only one thread/task at a time}
        \item{\texttt{flush} : this operation makes the thread's temporary view of memory consistent with the shared memory}
        \item{\texttt{ordered} : a structured block is executed in the order of the loop iterations }
\end{itemize}
\end{block}

\end{frame}


\begin{frame}[containsverbatim]
\frametitle{The \texttt{master} construct}

\begin{itemize}
        \item{Only the master thread execute the section. It can be used in any OpenMP construct}
\end{itemize}

\begin{lstlisting}[language=C,frame=lines]
#pragma omp parallel default(shared)
{
...
   #pragma omp master
   {
      printf("I am the master\n");
   }
...
}
\end{lstlisting}

\end{frame}

\subsubsection{Nesting}

\begin{frame}
\frametitle{Nesting regions}

\begin{exampleblock}{Nesting}
It is possible to include parallel regions in a parallel region (i.e. nesting) under restrictions (cf. sec. 2.10, p.111, \textit{OpenMP: Specifications ver. 3.1})
\end{exampleblock}


\end{frame}



\subsection{Runtime Library routines}

\begin{frame}
\frametitle{Runtime Library routines}
\begin{exampleblock}{Usage}
\begin{itemize}
\item{The functions/subroutines are defined in the lib \texttt{libomp.so / libgomp.so}. Don't
    forget to include \texttt{\#include <omp.h>}}
\item{These functions can be called anywhere in your programs}
\end{itemize}
\end{exampleblock}
\end{frame}



\subsubsection{Timing routines}

\begin{frame}
\frametitle{Runtime Library routines}
\framesubtitle{Timing routines}

\begin{center}
\begin{tabular}{|l|l|}
\hline
  \textbf{routine} & \textbf{behavior}  \\
\hline
\hline
\texttt{omp\_get\_wtime} &
returns elapsed wall clock time in seconds. \\
\hline
\texttt{omp\_get\_wtick} &
returns the precision of the timer used by \\
& \texttt{omp\_get\_wtime} \\
\hline
\end{tabular}
\end{center}

\end{frame}





\subsection{Environment variables}

\begin{frame}
\frametitle{Environment variables}
\begin{exampleblock}{Usage}
\begin{itemize}
\item{Environment variables are used to set the ICVs variables}
\item{under \texttt{csh} : \texttt{setenv OMP\_VARIABLE "its-value"}}
\item{under \texttt{bash} : \texttt{export OMP\_VARIABLE="its-value"}}
\end{itemize}
\end{exampleblock}
\end{frame}

\begin{frame}
\frametitle{Environment variables}

\begin{center}
\begin{tabular}{|l|l|}
\hline
  \textbf{variable} & \textbf{what for ?}  \\
\hline
\hline
\texttt{OMP\_SCHEDULE}
& sets the run-sched-var ICV that specifies \\
& the runtime schedule type and chunk size. \\
& It can be set to any of the valid OpenMP \\
& schedule types. \\
\hline

\texttt{OMP\_NUM\_THREADS}
& sets the nthreads-var ICV that specifies \\
& the number of threads to use in parallel  \\
& regions \\
\hline

\hline
\end{tabular}
\end{center}

\end{frame}

\subsubsection{The apparent ``easiness'' of OpenMP}

\begin{frame}
\frametitle{The apparent ``easiness'' of OpenMP}

\begin{block}{}
\textit{``Compared to MPI, OpenMP is much easier''}
\end{block}

\begin{exampleblock}{In the reality}
\begin{itemize}
\item{Parallelization of a non-appropriate algorithm}
\item{Parallelization of an unoptimized code}
\item{Race conditions in shared memory environment}
\item{Memory coherence}
\item{Compiler implementation of the OpenMP API}
\item{(Much) more threads/tasks than your machine can support}
\end{itemize}
\end{exampleblock}

\end{frame}


\subsection{About affinity}


\begin{frame}[fragile]
\frametitle{OpenMP Thread affinity}

\textbf{Affinity = on which core does my thread run ?}

\begin{block}{Show and set affinity with Intel executable}
By setting the \verb+export KMP_AFFINITY=verbose,SCHEDULING+ you are able to see where the OS pin each thread
\end{block}
\begin{block}{Show and set affinity with GNU executable}
By setting the \verb+export GOMP_CPU_AFFINITY=verbose,SCHEDULING+ you are able to see where the OS pin each thread
\end{block}
\end{frame}



\begin{frame}[containsverbatim]
  \frametitle{OpenMP Thread affinity with compact}
  \begingroup
  \fontsize{6pt}{12pt}\linespread{0.5}\selectfont
\begin{verbatim}
vkeller@mathicsepc13:~$ export KMP_AFFINITY=verbose,compact
vkeller@mathicsepc13:~$ ./ex10
OMP: Info #204: KMP_AFFINITY: decoding x2APIC ids.
OMP: Info #202: KMP_AFFINITY: Affinity capable, using global cpuid leaf 11 info
OMP: Info #154: KMP_AFFINITY: Initial OS proc set respected: {0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15}
OMP: Info #156: KMP_AFFINITY: 16 available OS procs
OMP: Info #157: KMP_AFFINITY: Uniform topology
OMP: Info #179: KMP_AFFINITY: 2 packages x 4 cores/pkg x 2 threads/core (8 total cores)
OMP: Info #206: KMP_AFFINITY: OS proc to physical thread map:
OMP: Info #171: KMP_AFFINITY: OS proc 0 maps to package 0 core 0 thread 0
OMP: Info #171: KMP_AFFINITY: OS proc 8 maps to package 0 core 0 thread 1
OMP: Info #171: KMP_AFFINITY: OS proc 1 maps to package 0 core 1 thread 0
OMP: Info #171: KMP_AFFINITY: OS proc 9 maps to package 0 core 1 thread 1
OMP: Info #171: KMP_AFFINITY: OS proc 2 maps to package 0 core 9 thread 0
OMP: Info #171: KMP_AFFINITY: OS proc 10 maps to package 0 core 9 thread 1
OMP: Info #171: KMP_AFFINITY: OS proc 3 maps to package 0 core 10 thread 0
OMP: Info #171: KMP_AFFINITY: OS proc 11 maps to package 0 core 10 thread 1
OMP: Info #171: KMP_AFFINITY: OS proc 4 maps to package 1 core 0 thread 0
OMP: Info #171: KMP_AFFINITY: OS proc 12 maps to package 1 core 0 thread 1
OMP: Info #171: KMP_AFFINITY: OS proc 5 maps to package 1 core 1 thread 0
OMP: Info #171: KMP_AFFINITY: OS proc 13 maps to package 1 core 1 thread 1
OMP: Info #171: KMP_AFFINITY: OS proc 6 maps to package 1 core 9 thread 0
OMP: Info #171: KMP_AFFINITY: OS proc 14 maps to package 1 core 9 thread 1
OMP: Info #171: KMP_AFFINITY: OS proc 7 maps to package 1 core 10 thread 0
OMP: Info #171: KMP_AFFINITY: OS proc 15 maps to package 1 core 10 thread 1
OMP: Info #144: KMP_AFFINITY: Threads may migrate across 1 innermost levels of machine
OMP: Info #147: KMP_AFFINITY: Internal thread 0 bound to OS proc set {0,8}
OMP: Info #147: KMP_AFFINITY: Internal thread 1 bound to OS proc set {0,8}
OMP: Info #147: KMP_AFFINITY: Internal thread 2 bound to OS proc set {1,9}
OMP: Info #147: KMP_AFFINITY: Internal thread 3 bound to OS proc set {1,9}
 [DGEMM]   Compute time [s]   :   0.344645023345947
 [DGEMM]   Performance  [GF/s]:   0.580307233391397
 [DGEMM]   Verification       :    2000000000.00000
\end{verbatim}
  \endgroup
\end{frame}



\begin{frame}[containsverbatim]
  \frametitle{OpenMP Thread affinity with scatter}
  \begingroup
  \fontsize{6pt}{12pt}\linespread{0.5}\selectfont
\begin{verbatim}
vkeller@mathicsepc13:~$ export KMP_AFFINITY=verbose,scatter
vkeller@mathicsepc13:~$ ./ex10
OMP: Info #204: KMP_AFFINITY: decoding x2APIC ids.
OMP: Info #202: KMP_AFFINITY: Affinity capable, using global cpuid leaf 11 info
OMP: Info #154: KMP_AFFINITY: Initial OS proc set respected: {0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15}
OMP: Info #156: KMP_AFFINITY: 16 available OS procs
OMP: Info #157: KMP_AFFINITY: Uniform topology
OMP: Info #179: KMP_AFFINITY: 2 packages x 4 cores/pkg x 2 threads/core (8 total cores)
OMP: Info #206: KMP_AFFINITY: OS proc to physical thread map:
OMP: Info #171: KMP_AFFINITY: OS proc 0 maps to package 0 core 0 thread 0
OMP: Info #171: KMP_AFFINITY: OS proc 8 maps to package 0 core 0 thread 1
OMP: Info #171: KMP_AFFINITY: OS proc 1 maps to package 0 core 1 thread 0
OMP: Info #171: KMP_AFFINITY: OS proc 9 maps to package 0 core 1 thread 1
OMP: Info #171: KMP_AFFINITY: OS proc 2 maps to package 0 core 9 thread 0
OMP: Info #171: KMP_AFFINITY: OS proc 10 maps to package 0 core 9 thread 1
OMP: Info #171: KMP_AFFINITY: OS proc 3 maps to package 0 core 10 thread 0
OMP: Info #171: KMP_AFFINITY: OS proc 11 maps to package 0 core 10 thread 1
OMP: Info #171: KMP_AFFINITY: OS proc 4 maps to package 1 core 0 thread 0
OMP: Info #171: KMP_AFFINITY: OS proc 12 maps to package 1 core 0 thread 1
OMP: Info #171: KMP_AFFINITY: OS proc 5 maps to package 1 core 1 thread 0
OMP: Info #171: KMP_AFFINITY: OS proc 13 maps to package 1 core 1 thread 1
OMP: Info #171: KMP_AFFINITY: OS proc 6 maps to package 1 core 9 thread 0
OMP: Info #171: KMP_AFFINITY: OS proc 14 maps to package 1 core 9 thread 1
OMP: Info #171: KMP_AFFINITY: OS proc 7 maps to package 1 core 10 thread 0
OMP: Info #171: KMP_AFFINITY: OS proc 15 maps to package 1 core 10 thread 1
OMP: Info #144: KMP_AFFINITY: Threads may migrate across 1 innermost levels of machine
OMP: Info #147: KMP_AFFINITY: Internal thread 0 bound to OS proc set {0,8}
OMP: Info #147: KMP_AFFINITY: Internal thread 1 bound to OS proc set {4,12}
OMP: Info #147: KMP_AFFINITY: Internal thread 2 bound to OS proc set {1,9}
OMP: Info #147: KMP_AFFINITY: Internal thread 3 bound to OS proc set {5,13}
 [DGEMM]   Compute time [s]   :   0.204235076904297
 [DGEMM]   Performance  [GF/s]:   0.979263714301724
 [DGEMM]   Verification       :    2000000000.00000
\end{verbatim}
\endgroup
\end{frame}


\begin{frame}[containsverbatim]
\frametitle{OpenMP Thread affinity with explicit (a kind of pining)}
\begingroup
    \fontsize{6pt}{12pt}\linespread{0.5}\selectfont
\begin{verbatim}
vkeller@mathicsepc13:~$ export KMP_AFFINITY='proclist=[0,2,4,6],explicit',verbose
vkeller@mathicsepc13:~$ ./ex10
OMP: Info #204: KMP_AFFINITY: decoding x2APIC ids.
OMP: Info #202: KMP_AFFINITY: Affinity capable, using global cpuid leaf 11 info
OMP: Info #154: KMP_AFFINITY: Initial OS proc set respected: {0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15}
OMP: Info #156: KMP_AFFINITY: 16 available OS procs
OMP: Info #157: KMP_AFFINITY: Uniform topology
OMP: Info #179: KMP_AFFINITY: 2 packages x 4 cores/pkg x 2 threads/core (8 total cores)
OMP: Info #206: KMP_AFFINITY: OS proc to physical thread map:
OMP: Info #171: KMP_AFFINITY: OS proc 0 maps to package 0 core 0 thread 0
OMP: Info #171: KMP_AFFINITY: OS proc 8 maps to package 0 core 0 thread 1
OMP: Info #171: KMP_AFFINITY: OS proc 1 maps to package 0 core 1 thread 0
OMP: Info #171: KMP_AFFINITY: OS proc 9 maps to package 0 core 1 thread 1
OMP: Info #171: KMP_AFFINITY: OS proc 2 maps to package 0 core 9 thread 0
OMP: Info #171: KMP_AFFINITY: OS proc 10 maps to package 0 core 9 thread 1
OMP: Info #171: KMP_AFFINITY: OS proc 3 maps to package 0 core 10 thread 0
OMP: Info #171: KMP_AFFINITY: OS proc 11 maps to package 0 core 10 thread 1
OMP: Info #171: KMP_AFFINITY: OS proc 4 maps to package 1 core 0 thread 0
OMP: Info #171: KMP_AFFINITY: OS proc 12 maps to package 1 core 0 thread 1
OMP: Info #171: KMP_AFFINITY: OS proc 5 maps to package 1 core 1 thread 0
OMP: Info #171: KMP_AFFINITY: OS proc 13 maps to package 1 core 1 thread 1
OMP: Info #171: KMP_AFFINITY: OS proc 6 maps to package 1 core 9 thread 0
OMP: Info #171: KMP_AFFINITY: OS proc 14 maps to package 1 core 9 thread 1
OMP: Info #171: KMP_AFFINITY: OS proc 7 maps to package 1 core 10 thread 0
OMP: Info #171: KMP_AFFINITY: OS proc 15 maps to package 1 core 10 thread 1
OMP: Info #144: KMP_AFFINITY: Threads may migrate across 1 innermost levels of machine
OMP: Info #147: KMP_AFFINITY: Internal thread 0 bound to OS proc set {0,8}
OMP: Info #147: KMP_AFFINITY: Internal thread 3 bound to OS proc set {6,14}
OMP: Info #147: KMP_AFFINITY: Internal thread 1 bound to OS proc set {2,10}
OMP: Info #147: KMP_AFFINITY: Internal thread 2 bound to OS proc set {4,12}
 [DGEMM]   Compute time [s]   :   0.248908042907715
 [DGEMM]   Performance  [GF/s]:   0.803509591990774
 [DGEMM]   Verification       :    2000000000.00000
\end{verbatim}
\endgroup
\end{frame}






\subsubsection{``OpenMP-ization'' strategy}


\begin{frame}
\frametitle{``OpenMP-ization'' strategy}

\begin{itemize}
\item {\textbf{STEP 1} : Optimize the sequential version:
        \begin{itemize}
        \item {Choose the best algorithm}
        \item {``Help the (right) compiler''}
        \item {Use the existing optimized scientific libraries}
        \end{itemize}
}
\item {\textbf{STEP 2} : Parallelize it:
        \begin{itemize}
        \item {Identify the bottlenecks (heavy loops)}
        \item {``auto-parallelization'' is rarely the best !}
        \end{itemize}
}
\end{itemize}

\begin{alertblock}{Goal}
Debugging - Profiling - Optimization cycle. Then parallelization !
\end{alertblock}


\end{frame}

\begin{frame}
\frametitle{Tricks and tips}

\begin{block}{}
\begin{itemize}
\item{\textbf{Algorithm}: choose the ``best'' one}
%\item{\textbf{Implementation}: choose the best one (remember Fibonacci)}
\item{\textbf{cc-NUMA}: no (real) support from OpenMP side (but OS). A multi-CPU machine is not a real shared memory architecture}
\item{\textbf{False-sharing}: multiple threads write in the same cache line}
\item{\textbf{Avoid barrier}. This is trivial. Bus sometimes you can't}
\item{\textbf{Small number of tasks}. Try to reduce the number of forked tasks}
\item{\textbf{Asymetrical problem}. OpenMP is well suited for symetrical problems, even if tasks can help}
\item{\textbf{Tune the schedule}: types, chunks...}
\item{\textbf{Performance expectations}: a theoretical analysis using the simple Amdahl's law can help}
\item{\textbf{Parallelization level}: coarse (SPMD) or fine (loop) grain ?}
\end{itemize}
\end{block}

\end{frame}



% =============================================================
% OpenMP 4.0
% =============================================================

\subsection{What's new in 4.0 ?}


\begin{frame}[fragile]
  \frametitle{What's new with OpenMP 4.0 ?}

  \begin{itemize}
  \item{ Support for new devices (\verb+Intel Phi+, \verb+GPU+,...) with \verb+omp target+. Offloading on those devices. }
  \item{ Hardware agnostic}
  \item{ League of threads with \verb+omp teams+ and distribute a loop over the team with \verb+omp distribute+ }
  \item{ SIMD support for vectorization \verb+omp simd+ }
  \item{ Task management enhancements (cancelation of a task, groups of tasks, task-to-task synchro)}
  \item{ Set thread affinity with a more standard way than \verb+KMP_AFFINITY+ with the concepts of \verb+places+ (a thread, a core, a socket), \verb+policies+ (spread, close, master) and \verb+control settings+ the new clause \verb+proc_bind+}
  \end{itemize}
\end{frame}


